import sys
import os
import re
from time import time
from typing import Tuple, Union
import logging.config
import pandas as pd
import numpy as np

from get_team_inventory_data import get_team_inventory_as_dataframe

from shared_var_and_constant import debug_pre_process, VA_REPORT_WRONG_CVE_VID, \
    REASON_INCONSISTENT_NUM_OF_VALUE_IN_CELLS, REASON_INCONSISTENT_PIPE_SEPARATORS, REASON_INVALID_IMAGE_NAME, \
    REASON_INVALID_CVE_VID_OR_BOTH, REASON_EMPTY_VALUE_ON_FOUND_IN_COLUMN
from lib.utility_lib import (
    check_python_and_pandas_version, colour_text, config_file_to_dictionary,
    create_working_dir_and_remove_tree_if_exists,
    get_base_image_clean_file_name, get_data_from_yaml_file,
    get_df_from_spreadsheet_file, get_file_from_dictionary,
    get_va_report_clean_file_name, strict_types, write_sheet_to_excel,
    MyColours)

# Configuration file
CONFIG_PROPERTY_FILE = './config_properties.yml'

# create logger
logging.config.dictConfig(get_data_from_yaml_file('logging.yaml'))
logger = logging.getLogger("InputTable")


@strict_types
def vulnerability_found_on_tool_check(va_repo_df: pd.DataFrame) -> pd.DataFrame:
    """
    Verify 'Found in <tool>' correct data values on columns
    Args:
        va_repo_df: vulnerability data frame

    Returns:
        vulnerability data frame with value on columns 'to_discard' and 'Reason' updated, if needed
    """
    column_tool_name = ['trivy', 'anchore_grype', 'xray']
    number_of_wrongly_lines = 0

    for tool in column_tool_name:
        found_in_column_name = f'Found in {tool}'
        condition = (pd.isnull(va_repo_df[found_in_column_name]) & (va_repo_df['to_discard'] == 'False'))

        va_repo_df.loc[condition, 'to_discard'] = 'True'
        va_repo_df.loc[condition, 'Reason'] = REASON_EMPTY_VALUE_ON_FOUND_IN_COLUMN

        number_of_wrongly_lines += len(va_repo_df.loc[condition])

    if number_of_wrongly_lines:
        logger.info("Lines to discard due to wrongly value on 'Found in <scan tool>' columns: "
                    f"{number_of_wrongly_lines}")

    return va_repo_df


@strict_types
def vulnerability_tools_check(support_tab: pd.DataFrame) -> pd.DataFrame:
    ##
    # vulnerability tools check
    esc_hashtag = re.escape('#')
    esc_asterisk = re.escape('*')
    support_tab.loc[support_tab['Found in trivy'] != 'no', 'FoundIn'] = support_tab['FoundIn'].map(str) + '*'
    support_tab.loc[support_tab['Found in anchore_grype'] != 'no', 'FoundIn'] = support_tab['FoundIn'].map(str) + '*'
    support_tab.loc[support_tab['Found in xray'] != 'no', 'FoundIn'] = support_tab['FoundIn'].map(str) + '*'

    found_in_cond = \
        (support_tab['FoundIn'].str.count(esc_asterisk) != support_tab['Rpm Package'].str.count(esc_hashtag) + 1) |\
        (support_tab['Rpm Package'].str.count(esc_hashtag) != support_tab['Package Paths'].str.count(esc_hashtag))

    support_tab.loc[found_in_cond, 'to_discard'] = 'True'

    support_tab.loc[found_in_cond, 'Reason'] = REASON_INCONSISTENT_NUM_OF_VALUE_IN_CELLS

    return support_tab


@strict_types
def packagepath_rpmpackage_check(support_tab: pd.DataFrame) -> pd.DataFrame:
    ##
    # Package Paths and Rpm Package consistency check
    separator_cond = (support_tab['Package Paths'].str.replace(r'[^\|#]', '', regex=True) !=
                      support_tab['Rpm Package'].str.replace(r'[^\|#]', '', regex=True))

    tab = support_tab.copy()
    tab.loc[separator_cond, 'to_discard'] = 'True'
    tab.loc[separator_cond, 'Reason'] = tab['Reason'] + ' ' + REASON_INCONSISTENT_PIPE_SEPARATORS
    tab['Reason'] = tab['Reason'].apply(lambda x: x.strip())
    return tab


@strict_types
def image_check(support_tab: pd.DataFrame) -> pd.DataFrame:
    ##
    # Image name consistency check
    image_cond = (support_tab['Image Name'].str.capitalize() == 'None') | \
                 (support_tab['Image Name'] == '') | (support_tab['Image Name'].isnull()) | \
                 (support_tab['Image Name'].isna())

    tab = support_tab.copy()
    tab.loc[image_cond, 'to_discard'] = 'True'
    tab.loc[image_cond, 'Reason'] = tab['Reason'] + ' ' + REASON_INVALID_IMAGE_NAME
    tab['Reason'] = tab['Reason'].apply(lambda x: x.strip())

    return tab


@strict_types
def cve_vid_check(support_tab: pd.DataFrame) -> pd.DataFrame:
    ##
    # CVE-REF VulnerabilityID consistency check

    #####################################################################################################
    # put in unprocessed all invalid case for CVE VID as described below:
    # remove all rows where the condition cve_vid_condition is satisfied:
    #      CVE=VID=None
    #      CVE=None and VID = vid_1 | vid_2
    #      VID=None and CVE = cve_1 | cve_2
    #      CVE=cve_1 |cve_2 and VID= vid_1 | vid_2
    #####################################################################################################

    # cve_cond = (support_tab['CVE-REF'] == 'None') | (support_tab['CVE-REF'] == 'none') | (
    #             support_tab['CVE-REF'] == '') | \
    #            (support_tab['CVE-REF'].isnull()) | (support_tab['CVE-REF'].isna()) | (
    #                support_tab['CVE-REF'].str.contains(' ')) | \
    #            (support_tab['CVE-REF'].str.contains('\|', regex=True))
    # vid_cond = (support_tab['VulnerabilityID'] == 'None') | (support_tab['VulnerabilityID'] == 'none') | \
    #            (support_tab['VulnerabilityID'] == '') | (support_tab['VulnerabilityID'].isnull()) | \
    #            (support_tab['VulnerabilityID'].isna()) | (support_tab['VulnerabilityID'].str.contains(' ')) | \
    #            (support_tab['VulnerabilityID'].str.contains('\|', regex=True))
    # cve_vid_cond = cve_cond & vid_cond

    support_tab.replace(
        {'CVE-REF': '\\|None', 'VulnerabilityID': '\\|None'}, {'CVE-REF': '', 'VulnerabilityID': ''},
        regex=True, inplace=True)

    escaped_pipe = re.escape('|')
    cve_cond = (support_tab['CVE-REF'].str.capitalize() == 'None') | (support_tab['CVE-REF'] == '') | (
        support_tab['CVE-REF'].isnull()) | (support_tab['CVE-REF'].isna()) | (
                   support_tab['CVE-REF'].str.contains(escaped_pipe, regex=True))
    vid_cond = (support_tab['VulnerabilityID'].str.capitalize() == 'None') | \
               (support_tab['VulnerabilityID'] == '') | (support_tab['VulnerabilityID'].isnull()) | \
               (support_tab['VulnerabilityID'].isna()) | \
               (support_tab['VulnerabilityID'].str.contains(escaped_pipe, regex=True))
    cve_vid_cond = cve_cond & vid_cond

    support_tab['to_discard'] = np.where(cve_vid_cond, 'True', support_tab['to_discard'])
    support_tab.loc[cve_vid_cond, 'Reason'] = support_tab['Reason'] + ' ' + REASON_INVALID_CVE_VID_OR_BOTH
    support_tab['Reason'] = support_tab['Reason'].apply(lambda x: x.strip())

    return support_tab


@strict_types
def get_intermediate_tables(support_tab: pd.DataFrame, discarded_rows_tab: pd.DataFrame):
    consistent_tab = support_tab
    if not debug_pre_process:
        consistent_tab = support_tab[support_tab['to_discard'] == 'False']
        discarded_rows_tab = pd.concat([discarded_rows_tab, support_tab[support_tab['to_discard'] == 'True']])
    return consistent_tab, discarded_rows_tab


##
# Pre check
@strict_types
def all_images_final_report_remove_unprocessing(va_scan_support_tab: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
    logger.debug(f"{va_scan_support_tab.count}")
    va_scan_support_tab['FoundIn'] = ''
    va_scan_support_tab['Reason'] = ''
    va_scan_support_tab['to_discard'] = 'False'

    ##
    # Removing the beginning and ending spaces
    va_scan_support_tab['Image Name'].replace(to_replace=r'^\s+|\s+$', value='', regex=True, inplace=True)
    va_scan_support_tab['CVE-REF'].replace(to_replace=r'^\s+|\s+$', value='', regex=True, inplace=True)
    va_scan_support_tab['VulnerabilityID'].replace(to_replace=r'^\s+|\s+$', value='', regex=True, inplace=True)
    va_scan_support_tab['Package Paths'].replace(to_replace=r'^\s+|\s+$', value='', regex=True, inplace=True)

    # Checks and Detailed Reason Traces
    on_going_discarded_rows = pd.DataFrame()

    checked_tab = vulnerability_found_on_tool_check(va_scan_support_tab)
    on_going_va_scan_support_tab, on_going_discarded_rows = get_intermediate_tables(checked_tab,
                                                                                    on_going_discarded_rows)

    checked_tab = vulnerability_tools_check(va_scan_support_tab)
    on_going_va_scan_support_tab, on_going_discarded_rows = get_intermediate_tables(checked_tab,
                                                                                    on_going_discarded_rows)

    checked_tab = packagepath_rpmpackage_check(on_going_va_scan_support_tab)
    on_going_va_scan_support_tab, on_going_discarded_rows = get_intermediate_tables(checked_tab,
                                                                                    on_going_discarded_rows)

    checked_tab = image_check(on_going_va_scan_support_tab)
    on_going_va_scan_support_tab, on_going_discarded_rows = get_intermediate_tables(checked_tab,
                                                                                    on_going_discarded_rows)

    checked_tab = cve_vid_check(on_going_va_scan_support_tab)
    on_going_va_scan_support_tab, on_going_discarded_rows = get_intermediate_tables(checked_tab,
                                                                                    on_going_discarded_rows)

    if not debug_pre_process:
        df_va_report_consistent_rows = on_going_va_scan_support_tab
        df_va_report_discarded_rows = on_going_discarded_rows
    else:
        df_va_report_consistent_rows = on_going_va_scan_support_tab[
            on_going_va_scan_support_tab['to_discard'] == 'False']
        df_va_report_discarded_rows = on_going_va_scan_support_tab[on_going_va_scan_support_tab['to_discard'] == 'True']

    df_va_report_consistent_rows = df_va_report_consistent_rows.drop(['FoundIn'], axis=1)
    df_va_report_discarded_rows = df_va_report_discarded_rows.drop(['FoundIn'], axis=1)

    logger.debug(f"Output VA report cleaned {df_va_report_consistent_rows.count}")
    logger.debug(f"Discarded VA report rows {df_va_report_discarded_rows.count}")

    return df_va_report_consistent_rows, df_va_report_discarded_rows


@strict_types
def replace_row_value(dataframe: pd.DataFrame, value_list: list, col: str):
    lower_list = [elem.lower() for elem in value_list]
    df_nan_rows = dataframe[dataframe[col].isnull()]
    dataframe = dataframe[~dataframe[col].isnull()]
    logger.debug("LEN TABLE without null element %s", len(dataframe))
    logger.debug("LEN TABLE null element %s", len(df_nan_rows))
    logger.debug(f"{df_nan_rows.count}")
    for row_index, row in dataframe.iterrows():
        row_str = row[col]
        if row_str.lower() in lower_list:
            index = lower_list.index(row[col].lower())
            dataframe.at[row_index, col] = value_list[index]
        else:
            dataframe.at[row_index, col] = row_str
    df_all = pd.concat([dataframe, df_nan_rows], ignore_index=True)
    return df_all


def load_input_tables():
    sg_mapping_table_df = get_df_from_spreadsheet_file(sg_mapping_table_file_name)
    cxp_table_df = get_df_from_spreadsheet_file(cxp_table_file_name)
    team_inv_table_df = get_df_from_spreadsheet_file(team_inv_table_file_name)
    scan_report_table_df = get_df_from_spreadsheet_file(scan_report_table_file_name)
    base_image_table_df = get_df_from_spreadsheet_file(base_image_table_file_name)

    return sg_mapping_table_df, cxp_table_df, team_inv_table_df, scan_report_table_df, base_image_table_df


def clean_sg_mapping_table():
    # generate an SG mapping table without duplicates and save it for further analysis
    sg_table = sg_mapping_table.copy()
    sg_table = sg_table.drop_duplicates(subset='SG List from VA Scan', keep="first")
    return sg_table


def clean_cxp_table():
    # generate a CXP mapping table without duplicates and save it for further analysis
    cxp_table_cp = cxp_table.copy()
    cxp_table_cp = cxp_table_cp[~cxp_table_cp['CXP Name'].str.strip().str.lower().duplicated(keep='first')]
    return cxp_table_cp


@strict_types
def get_input_files(input_data_dictionary: dict) -> Tuple[Union[str, None], Union[str, None], Union[str, None],
                                                          Union[str, None], Union[str, None]]:
    if input_data_dictionary is not None:
        sg_mapping_table_file = get_file_from_dictionary(input_data_dictionary, 'sg_mapping_table')
        scan_report_table_file = get_file_from_dictionary(input_data_dictionary, 'va_report')
        cxp_table_file = get_file_from_dictionary(input_data_dictionary, 'cxp_table')
        team_inv_table_file = get_file_from_dictionary(input_data_dictionary, 'team_inv_table')
        base_image_table_file = get_file_from_dictionary(input_data_dictionary, 'base_image')

        return cxp_table_file, sg_mapping_table_file, team_inv_table_file, scan_report_table_file, \
            base_image_table_file

    return None, None, None, None, None


@strict_types
def check_and_extract_cxp(cxp_field: str) -> str:
    """
    Check and returns a 'cleaned' CXP value or the passed value if an issue is found
    Args:
        cxp_field: string containing a CXP value

    Returns:
        If only one valid CXP is found, that value is returned.
        Otherwise, log the error and returns the passed value.
    """
    if cxp_field == 'None':
        logger.error("CXP field is 'None'!")
        return cxp_field

    cxp_field = cxp_field.replace(' ', '')
    cxp_field = cxp_field.upper()
    cxp = re.findall('CXP\\d+', cxp_field)
    if len(cxp) == 1:  # If only one valid CXP is found, that value is returned
        return cxp[0]

    logger.error(f"CXP field '{cxp_field}' doesn't contain a unique valid CXP item!")
    return cxp_field


# -------------------------------------
# check pandas and python version
# ------------------------------------

start_time = time()
logger.debug("Script: %s", os.path.basename(__file__))
logger.debug("Used pandas version: %s", pd.__version__)
logger.debug("Used python version: %s.%s.%s", sys.version_info.major,
             sys.version_info.minor, sys.version_info.micro)
check_python_and_pandas_version(str(sys.version_info.major), "3",
                                str(pd.__version__), "2")

# --------------------------------------------
# Read config file input data into dictionary
# -------------------------------------------
logger.info("Loading configuration from properties file: %s ....", CONFIG_PROPERTY_FILE)
dict_from_config_file = config_file_to_dictionary(CONFIG_PROPERTY_FILE, 'csv_generator_properties')

# -------------------------------------
# Adjust Windows filenames for Linux
# ------------------------------------
cxp_table_file_name, sg_mapping_table_file_name, team_inv_table_file_name, scan_report_table_file_name, \
    base_image_table_file_name = get_input_files(dict_from_config_file)

logger.debug("sg_mapping_table file:   %s ", sg_mapping_table_file_name)
logger.debug("cxp_table file:          %s", cxp_table_file_name)
logger.debug("team_inv_table file:     %s", team_inv_table_file_name)
logger.debug("scan_report_table file:  %s", scan_report_table_file_name)
logger.debug("base_image_table file:   %s", base_image_table_file_name)

# -------------------------------------
# create working directory
# ------------------------------------
try:
    create_working_dir_and_remove_tree_if_exists('clean_input_tables')
    clean_table_dir = create_working_dir_and_remove_tree_if_exists('clean_db_input_tables')
    supp_table_dir = create_working_dir_and_remove_tree_if_exists('generated_support_tables')
    input_table_dir = create_working_dir_and_remove_tree_if_exists('input_tables', False)

except OSError as error:
    logging.error(colour_text(MyColours.RED,
                              f"Working directory creation failed. Reason : {error}"))
    sys.exit(1)

output_table_name = get_va_report_clean_file_name()
base_image_name = get_base_image_clean_file_name()


# -------------------------------------
# load table into dataframe struct
# ------------------------------------
sg_mapping_table, cxp_table, team_inv_table, scan_report_table, base_image_table = load_input_tables()
if team_inv_table is None:
    #     script launched standalone : download team inventory
    team_inv_table = get_team_inventory_as_dataframe()

# -------------------------------------
# fix db input table errors (duplicated rows and other).
# For further details see SG_CXP_duplicated_lines.xlsx
# ------------------------------------
if sg_mapping_table is not None and cxp_table is not None and team_inv_table is not None \
        and scan_report_table is not None and base_image_table is not None:
    # temporary fix the problem of sg_mapping_table wrong info,
    # align team name to team inventory table for all input table
    sg_mapping_table = clean_sg_mapping_table()
    logger.debug(f"{cxp_table.count}")

    team_name = team_inv_table['Team'].sort_values().to_list()
    logger.debug("'Team' name from Team Inventory table:\n\t - %s\n", "\n\t - ".join(team_name))
    cxp_team_name = cxp_table['CXP Resp.Team'].to_list()
    cxp_table.to_excel(os.path.join(supp_table_dir, 'before_pre_process_CXP_Table.xlsx'))
    logger.info("Replacing team names in CXP table .......")
    cxp_table = replace_row_value(cxp_table, team_name, 'CXP Resp.Team')
    cxp_table['CXP'] = cxp_table['CXP'].apply(lambda x: check_and_extract_cxp(x))
    cxp_table.to_csv('clean_' + get_file_from_dictionary(dict_from_config_file, 'cxp_table'), index=False)
    sg_mapping_table.to_excel(os.path.join(supp_table_dir, 'before_pre_process_SG_mapping_Table.xlsx'))
    logger.info("Replacing team names in SG Mapping table .......")
    sg_mapping_table = replace_row_value(sg_mapping_table, team_name, 'Team from PRM')
    sg_mapping_table.drop(['Team email', 'CNA Owner', 'Faulty CRA', 'Faulty CNA'], axis=1, inplace=True)
    sg_mapping_table.to_csv('clean_' + get_file_from_dictionary(dict_from_config_file, 'sg_mapping_table'))

    # ------------------------------------------------------------------------------------
    # VA scan report pre-processing: placeholder for VA scan report cleanup (to be added)
    # ------------------------------------------------------------------------------------
    logger.info("Pre-processing VA report start......")
    scan_report_cleaned, unprocessed_entries_df = \
                    all_images_final_report_remove_unprocessing(scan_report_table)
    base_image_cleaned, base_unprocessed_entries_df = \
                    all_images_final_report_remove_unprocessing(base_image_table)

    logger.info("Generating clean VA report in %s ......", output_table_name)
    scan_report_cleaned.to_csv(output_table_name)
    logger.info("Generating clean base image in %s ......", base_image_name)
    base_image_cleaned.to_csv(base_image_name)
    #  if len(base_unprocessed_entries_df) > 0:
    write_sheet_to_excel(os.path.join(supp_table_dir, "unprocessed_base_image.xlsx"),
                         base_unprocessed_entries_df)
    write_sheet_to_excel(os.path.join('./input_tables', VA_REPORT_WRONG_CVE_VID),
                         unprocessed_entries_df)
    end_time = time()
    execution_time = end_time - start_time
    logger.info(colour_text(
        MyColours.YELLOW, f'Execution time: {execution_time: .2f}s'))
else:
    logger.error(colour_text(
        MyColours.RED,
        f"something went wrong with input tables in {os.path.basename(sys.argv[0])}....exiting"))
    sys.exit(1)
