"""
    Utility functions library for reading excel and csv files, pandas df merge operation,
    excel output format file generator, etc. not specific for this application
    but general purpose
"""
import os
import re
import shutil
import sys
import logging
from typing import Tuple, Union
from typing import get_type_hints

import numpy as np
import pandas as pd
import yaml

####################################################
# CONSTANT DEFINITION
####################################################
CONFIG_PROPERTY_FILE = 'config_properties.yml'


class MyColours:
    """
        Enumerated to display text in different colours, replacing the default colour
        at the end with ENDC value
    """
    RED = '\033[1;31m'
    PINK = '\033[95m'
    ENDC = '\033[1;m'
    GREEN = '\033[1;32m'
    YELLOW = '\033[1;33m'
    BLUE = '\033[1;34m'
    BOLD = '\033[1m'


MAX_COLUMN_LENGTH = 125
SELECTOR_SIZE = 5

# create logger
logger = logging.getLogger("Utility_lib")


def strict_types(function_to_check):
    """
    Strict function parameter type checker. To be added using @strict_type annotation
    It doesn't validate return type.
    Arguments:
        function_to_check: function name
    Returns:
        None if parameter type match the declared one, Throw exception otherwise
    """

    def type_checker(*args, **kwargs):
        hints = get_type_hints(function_to_check)

        all_args = kwargs.copy()
        all_args.update(dict(zip(function_to_check.__code__.co_varnames, args)))

        for _, value in enumerate(all_args):
            if value in hints and not isinstance(all_args[value], hints[value]):
                raise Exception(f"Type of '{value}' is '{hints[value]}' and not '{type(all_args[value])}'")
        return function_to_check(*args, **kwargs)

    return type_checker


@strict_types
def get_data_from_yaml_file(config_file_name: str) -> Union[dict, None]:
    """
    Load a yaml file and return a dictionary or None in case of error
    Arguments:
        config_file_name:  full path of the yaml file
    Returns:
        None if config_file_name is not a valid yaml file , the yaml file otherwise
    """
    with open(config_file_name, "r", encoding='UTF-8') as stream:
        try:
            return yaml.safe_load(stream)
        except yaml.YAMLError:
            logger.exception(colour_text(MyColours.RED, 'YAMLError exception: \n'))
    return None


@strict_types
def get_df_from_spreadsheet_file(path: str) -> Union[pd.DataFrame, None]:
    """
    Read into a pandas dataframe the content of xlsx file and return it
    Arguments:
        path: Full path name of the input Excel file
    Returns:
        None - If the input file doesn't exist or the file format is not the expected one
        Dataframe - if the input files was correctly read
    """
    df_to_return = None
    if os.path.exists(path):
        try:
            default_missing = pd._libs.parsers.STR_NA_VALUES
            if 'None' in default_missing:
                default_missing.remove('None')

            df_to_return = pd.read_csv(path, na_values=list(default_missing)) if path.endswith('.csv') \
                else pd.read_excel(path, na_values=list(default_missing))
            # df_to_return = pd.read_csv(path) if path.endswith('.csv') else pd.read_excel(path)

            logger.info("file %s read!", path)
        except Exception:
            logger.error(colour_text(MyColours.RED, f'invalid file type : {path}'))
            logger.exception(colour_text(MyColours.RED, 'Exception details: \n'))
    else:
        logger.error(colour_text(MyColours.RED, f"File {path} doesn't exist!"))

    return df_to_return


@strict_types
def remove_rows(prev_df: pd.DataFrame, col_name: str, value: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    Remove rows where the value in the specified column match a string
    Arguments:
        prev_df: pandas Dataframe from which to remove
        col_name: string name of the column header
        value: string to search in column to identify rows to remove
    Returns:
        None - If the input file doesn't exist or the file format is not the expected one
        Dataframe - if the input files was correctly read
    """
    condition_to_remove = prev_df[col_name] == value
    new_df = prev_df[~condition_to_remove]
    removed_df = prev_df[condition_to_remove]
    return new_df, removed_df


@strict_types
def remove_rows_by_listvalues(prev_df: pd.DataFrame, col_name: str, valuelist: list) -> pd.DataFrame:
    """
    Remove rows where the value in the specified column match values in list
    Arguments:
        prev_df: pandas Dataframe from which to remove
        col_name: string name of column label
        valuelist: list of string to search in column to identify rows to remove
    Returns:
        DataFrame without the removed index or column labels or None if inplace=True.
    """
    new_df = prev_df.drop(prev_df[prev_df[col_name].isin(valuelist)].index)
    return new_df


@strict_types
def filter_table_by(df_to_filter: pd.DataFrame, column: str, valuelist: list) -> pd.DataFrame:
    """
    Filter a pandas Dataframe according to a specific value present in the specified column
    Arguments:
        df_to_filter: pandas Dataframe to filter
        column: str name of column label
        valuelist: list of string to filter by
    Returns:
        DataFrame resulting after filtering by valuelist.
    """
    new_tab = df_to_filter.loc[df_to_filter[column].isin(valuelist)]
    return new_tab


@strict_types
def validate_table_output(df1: pd.DataFrame, col1: str, df2: pd.DataFrame, col2: str) -> bool:
    """
    Filter a pandas Dataframe according to a specific value present in the specified column
    Arguments:
        df1: pandas Dataframe
        col1: str, column label of df1 to consider for validation
        df2: pandas Dataframe
        col2: str, column label of df2 to consider for validation
    Returns:
        bool, True/False
    """
    # this function check the values of rows in a specific column are the same number of occurrences
    # as in the second dataframe
    # to verify , for example, the two dataframe return identical number of rows for each SG
    compare = df1[col1].value_counts().equals(df2[col2].value_counts())
    return compare


@strict_types
def validate_table_len(df1: pd.DataFrame, df2: pd.DataFrame) -> bool:
    """
    Compare two pandas Dataframe to verify they have the same number of rows
    Arguments:
        df1: pandas DataFrame - the resulting dataframe after rows removal
        df2: pandas DataFrame - the input dataframe
    Returns:
        bool, True/False
    """
    return len(df1) == len(df2)


@strict_types
def validate_remove_rows_by_listvalues(new_df: pd.DataFrame, old_df: pd.DataFrame, col_name: str,
                                       list_of_values: list, name_new_df: str, name_old_df: str) -> bool:
    """
    validate a dataframe against removed rows matching pair (col_name column label, column label in
    list_of_values). Validate no occurrences of any string in list_of_values is present in column
    with label col_name in the final table.

    Arguments:
        new_df: a pandas DataFrame - the resulting dataframe after rows removal
        old_df: a pandas DataFrame - the input dataframe
        col_name: str, the column label
        list_of_values: list of values in column label to match for removing rows
        name_new_df: name of the new table name
        name_old_df: name of the old table name

    Returns:
        bool, True/False
    """
    removed_elem_len = len(old_df.loc[old_df[col_name].isin(list_of_values)])
    logger.info('Removed element length = %d ', removed_elem_len)
    if len(old_df) - len(new_df) != len(old_df[old_df[col_name].isin(list_of_values)].index):
        logger.error("ERROR!: new table %s doesn't match previous table %s "
                     "len considering len of removed elem %d ... EXITING",
                     name_new_df, name_old_df, removed_elem_len)
        return False

    if new_df[col_name].isin(list_of_values).any().any():
        logger.error("ERROR!: new table %s still presents %s values ... EXITING", name_new_df, list_of_values)
        return False
    return True


@strict_types
def validate_remove_rows(new_df: pd.DataFrame, old_df: pd.DataFrame, col_name: str, value: str,
                         name_new_df: str, name_old_df: str) -> bool:
    """
    validate a dataframe against removed rows matching pair (column label column name, column value).
    Validate no occurrence of value is present in column with label col_name in the final table.
    Arguments:
        new_df: pandas DataFrame - the resulting dataframe after rows removal
        old_df: pandas DataFrame - the input dataframe
        col_name: str, the column label
        value: str, the value in column label to match for removing rows
        name_new_df: name of the new table name
        name_old_df: name of the old table name
    Returns:
        bool, True/False
    """
    logger.info(len(old_df.loc[old_df[col_name] == value]))
    if len(old_df) - len(new_df) != len(old_df[old_df[col_name] == value].index):
        logger.error(
            "ERROR!: new table %s doesn't match previous table %s len "
            "considering len of removed elem %d {}  ... EXITING", name_new_df, name_old_df,
            len(old_df.loc[old_df[col_name] == value]))
        return False

    if value in new_df[col_name].unique():
        logger.error("ERROR!: new table %s still presents %s values  ... EXITING", name_new_df, value)
        return False
    return True


@strict_types
def create_working_dir_and_remove_tree_if_exists(working_dir_name: str, remove_dirtree_if_exist: bool = True) -> str:
    """
    create a directory. If the directory already exist and remove_dirtree_if_exist = True
    the directory an all the files in are deleted.
    Arguments:
        working_dir_name: a string - just directory name to be created in current execution directory
        remove_dirtree_if_exist: an integer
    Returns:
        The newly created directory or the existing one
    """
    new_dir = os.path.join(os.getcwd(), working_dir_name)
    if os.path.exists(new_dir) and os.path.isdir(new_dir):
        if remove_dirtree_if_exist:
            clean_dir_tree(new_dir)

    os.makedirs(new_dir, exist_ok=True)
    return new_dir


@strict_types
def clean_dir_tree(new_dir: str):
    """
    Remove all files (directory tree) specified.
    Arguments:
        new_dir: a string - just directory name to be created in current execution directory
    Returns:
        None
    """
    shutil.rmtree(new_dir, ignore_errors=True)


@strict_types
def set_postfix_file_name(enm_version: str, datetime: str) -> str:
    """
    Add date and time at the end of a string after '_'.
    Arguments:
        datetime: date and time value
        enm_version: a string - enm version string
    Returns:
        the enhanced enm version string
    """
    return enm_version + "_" + datetime


@strict_types
def extract_string_by_regex(string_to_match: str, regex: str) -> str:
    """
    extract from string_match the first matching using re.findall and regex. If no match is found
    the original string is returned
    Arguments:
        string_to_match: a string - enm version string
        regex: str, regular expression to match a unique substring using re.findall
    Returns:
        the matching substring, if any, or the original one
    """
    if string_to_match != 'None':
        matching = re.findall(regex, string_to_match)
        if matching:
            return matching[0]
    return 'None'


@strict_types
def intersect_df_by_multiple_column(new_df: pd.DataFrame, removed_df: pd.DataFrame, col_list: list) -> int:
    """
    intersect two dataframes by multiple COMMON columns in col_list (it can be used to verify rows removal)
    Arguments:
        new_df: a pd.DataFrame - the resulting df after removal of all common rows with removed_df
        removed_df: a pd.DataFrame - the df to be removed from new_df on col_list basis
        col_list: list - the list of common columns to match between the two dfs for removal
    Returns:
        the resulting length of df grouped by col_list
    """
    count_series = new_df.groupby(col_list).size()
    df1 = count_series.to_frame(name='size').reset_index()
    count_series = removed_df.groupby(col_list).size()
    df2 = count_series.to_frame(name='size').reset_index()
    merged_df = pd.merge(df1, df2, how='inner', on=col_list)
    logger.debug(df1.count)
    logger.debug(df2.count)
    return len(merged_df)


@strict_types
def swap(df_to_swap: pd.DataFrame, col_a: str, col_b: str, sep: str) -> pd.DataFrame:
    """
    swap the value in one row on col_a with the value in col_b if the value in col_a
    contains sep (separator)
    Arguments:
        df_to_swap: pandas Dataframe whose row values need to be swapped if condition (separator presence)
                    is satisfied
        col_a: pandas Dataframe the column label whose row value as to be swapped with col_b value
         if condition is satisfied
        col_b: pandas Dataframe the column label whose row value as to be swapped with col_a value
         if condition is satisfied
        sep: str , the separator to search for
    Returns:
        Update pandas DataFrame if any swap is applied, otherwise the original dataframe
    """
    for row_index, row in df_to_swap.iterrows():
        if len(row[col_a].split(sep)) > 1:
            # swap
            col_a_value = row[col_a]
            col_b_value = row[col_b]
            df_to_swap.at[row_index, col_a] = col_b_value
            df_to_swap.at[row_index, col_b] = col_a_value
    return df_to_swap


@strict_types
def get_file_from_dictionary(config_dict: dict, key: str) -> Union[str, None]:
    """
    normalize a filename in the config yaml file
    Arguments:
        config_dict: str - the configuration yaml file
        key: str, the dictionary header of yaml file
    Returns:
        the normalized path of the file, None in case of error
    """
    filename = None
    if key in config_dict:
        if config_dict[key] is not None:
            filename = os.path.normpath(config_dict[key])
    else:
        logger.error("ERROR! key %s doesn't exists", key)

    return filename


@strict_types
def config_file_to_dictionary(cnf_file: str, yaml_header: str) -> dict:
    """
    read a yaml file into a python dictionary
    Arguments:
        cnf_file: str - the configuration yaml file
        yaml_header: str, the dictionary header of yaml file
    Returns:
        the dictionary containing config file information
    """
    input_dict = None
    script_input_data = get_data_from_yaml_file(cnf_file)
    # previous OLD name util.load_pkg_to_rpm_associations, better to rename get_dict_from_yaml
    if script_input_data is not None:
        input_dict = script_input_data[yaml_header]

    return input_dict


@strict_types
def remove_rows_matching_on_columns_value(left_df: pd.DataFrame, right_df: pd.DataFrame,
                                          left_column_names: list,
                                          right_column_names: list, destination_dir: str,
                                          save_to_file: str = 'delta_df_cols_diff.xlsx',
                                          debug_flag: bool = False) -> pd.DataFrame:
    """
    remove matching rows in two dataframe (left_df and right_df) by columns respectively in left_common_names and
    right_column_names (it can be used to verify rows removal)
    Arguments:
     left_df: a pd.Dataframe - the main dataframe whose rows are to be preserved
     right_df: a pd.Dataframe, the right dataframe, whose rows as to be removed from left_df
     left_column_names: names of left columns to match with
     right_column_names:names of right columns to match with
     destination_dir: destination directory where to save output files generated from dfs
     save_to_file: optional filename where to save data, default delta_df_cols_diff.xlsx
     debug_flag: optional flag for saving additional output files for debug purpose (merge output file)

    Returns:
      the matching Dataframe
    """
    if debug_flag:
        logger.debug("Matching rows contents on left/right columns values respectively %s %s ",
                     left_column_names,
                     right_column_names)
    delta_df = left_df.merge(right_df,
                             left_on=left_column_names,
                             right_on=right_column_names,
                             how='left',
                             indicator=True,
                             suffixes=('', '_drop'))
    if debug_flag:
        delta_df.to_excel(os.path.join(destination_dir, 'merge_' + save_to_file))
    delta_df = left_df.merge(right_df, left_on=left_column_names, right_on=right_column_names,
                             how='left',
                             indicator=True,
                             suffixes=('', '_drop')).query('_merge=="left_only"').drop('_merge', axis=1)
    delta_df = delta_df.filter(regex=r'^(?!(.*_drop|Unnamed:.*$))')
    delta_df.to_excel(os.path.join(destination_dir, 'result_' + save_to_file))
    if debug_flag:
        logger.debug("delta_df %s", delta_df.count)
    return delta_df


@strict_types
def colour_text(color: str, message: str) -> str:
    """
    Add colour to a text message to display with logger

    Returns:
        the coloured string to display
    """
    return color + message + MyColours.ENDC


@strict_types
def explode_rows_with_separator(df_to_explode: pd.DataFrame, col_name: str, separator: str) -> pd.DataFrame:
    """
    Split, using a separator, the column name rows and explode the content
    Args:
        df_to_explode: input data frame
        col_name: column name of the dataframe where apply the 'explode' method
        separator: the separator character

    Returns:
        the exploded dataframe
    """
    return df_to_explode.assign(**{col_name: df_to_explode[col_name].str.split(separator)}).explode(col_name,
                                                                                                    ignore_index=True)


@strict_types
def add_columns_from_a_df_to_another_df(df_to_complete, df_columns_to_add, left_ref_col: list,
                                        right_ref_col: list):
    """
    Merge (Add) df_to_complete columns from df_columns_to_add using as matching column
    left_ref_col and right_ref_col

    Args:
        df_to_complete: input data frame
        df_columns_to_add: dataframe containing columns information to add to input dataframe
        left_ref_col: the list of columns in input dataframe to use for merging
        right_ref_col: the list of columns in cxp_df which has the same content as left_ref_col

    Returns:
        the resulting merged dataframe
    """
    df_to_complete = pd.merge(df_to_complete, df_columns_to_add, how='left',
                              left_on=left_ref_col,
                              right_on=right_ref_col)
    df_to_complete.replace(np.nan, 'None', regex=True, inplace=True)
    return df_to_complete


@strict_types
def get_va_report_clean_file_name() -> Union[str, None]:
    """
    Get the filename of the va scan report cleaned by wrong rows

    Returns:
        the filename or None if the key doesn't exist
    """
    va_report_filepath = get_va_report()
    if va_report_filepath is not None:
        return 'clean_' + os.path.splitext(os.path.normpath(get_va_report()))[0] + '.csv'
    return None


@strict_types
def get_base_image_clean_file_name() -> Union[str, None]:
    """
    Get the filename of the base image report cleaned by wrong rows

    Returns:
        the filename or None if the key doesn't exist
    """
    base_image_filepath = get_base_image()
    if base_image_filepath is not None:
        return 'clean_' + os.path.splitext(os.path.normpath(base_image_filepath))[0] + '.csv'
    return None


@strict_types
def get_config_properties_value_from_key(data_tag: str) -> Union[list, str, int, bool, None]:
    """
    return the value in config_properties.yml file matching the key = data_tag.
    Args:
        data_tag: key to search in config_properties.yml file

    Returns:
        None whenever the searched key doesn't exist, the value otherwise.
        The type of the returned value can vary and could require function update as soon
        as return type checking will be available
    """
    config_file_name = os.path.realpath(
        os.path.join(os.path.dirname(__file__), '..', '..', CONFIG_PROPERTY_FILE))
    dict_from_config_file = config_file_to_dictionary(config_file_name, 'csv_generator_properties')
    return dict_from_config_file.get(data_tag)


@strict_types
def get_severities() -> Union[list, None]:
    """
    Get from config_properties.yml file the severities to filter by

    Returns:
        the severities value or None if the key doesn't exist
    """
    return get_config_properties_value_from_key('default_severities')


@strict_types
def get_debug_va_repo_enriched() -> Union[bool, None]:
    """
    Get from config_properties.yml file the va enriched report name

    Returns:
        the severities va enriched report name or None if the key doesn't exist
    """
    return get_config_properties_value_from_key('debug_va_repo_enriched')


@strict_types
def get_debug_pre_process() -> Union[bool, None]:
    """
    Get from config_properties.yml file the debug flag for pre-process script

    Returns:
        the debug flag of pre-process script value or None if the key doesn't exist
    """
    return get_config_properties_value_from_key('debug_pre_process')


@strict_types
def get_timeout() -> Union[int, None]:
    """
    Get from config_properties.yml file the execution timeout set

    Returns:
        the execution timeout value or None if the key doesn't exist
    """
    value = get_config_properties_value_from_key('exec_timeout')
    if value is not None and isinstance(value, int):
        return get_config_properties_value_from_key('exec_timeout')
    return int(0)


@strict_types
def get_enm_product_set() -> Union[str, None]:
    """
    Get from config_properties.yml file the execution timeout set

    Returns:
        the enm product set value or None if the key doesn't exist
    """
    return get_config_properties_value_from_key('enm_product_set')


@strict_types
def get_va_report() -> Union[str, None]:
    """
    Get from config_properties.yml file the va report filename

    Returns:
        the va report filename value or None if the key doesn't exist
    """
    value = get_config_properties_value_from_key('va_report')
    if value is not None and not isinstance(value, str):
        return None
    return value


@strict_types
def get_base_image() -> Union[str, None]:
    """
    Get from config_properties.yml file the base image filename

    Returns:
        the base image filename value or None if the key doesn't exist
    """
    value = get_config_properties_value_from_key('base_image')
    if value is not None and not isinstance(value, str):
        return None
    return value


@strict_types
def get_generate_jira_tickets() -> Union[bool]:
    """
    Get from config_properties.yml file the generate_jira_tickets

    Returns:
        the generate_jira_tickets value or False if the key doesn't exist or is invalid
    """
    value = get_config_properties_value_from_key('generate_jira_tickets')
    if value is not None and isinstance(value, bool):
        return value
    return False


@strict_types
def get_update_cxp_table_with_prm_data() -> Union[bool, None]:
    """
    Get from config_properties.yml file the update_cxp_table_with_prm_data

    Returns:
        the update_cxp_table_with_prm_data value or False if the key doesn't exist or is invalid
    """
    value = get_config_properties_value_from_key('update_cxp_table_with_prm_data')
    if value is not None and isinstance(value, bool):
        return value
    return False


@strict_types
def df_diff(df_left: pd.DataFrame, df_right: pd.DataFrame, how: str,
            remove_duplicates: bool,
            columns_to_exclude: list) -> pd.DataFrame:
    """
    Calculate diff from data frame (Tested only: left, right, inner)
    Args:
        df_left: dataframe left
        df_right: dataframe right
        how: type of merge operation (left, right, inner ...)
        remove_duplicates: True if duplicate row must be removed
        columns_to_exclude: list of columns to exclude before merge

    Returns:
        merged data frame
    """

    # Remove excluded columns
    df_left = df_left.drop(columns_to_exclude, axis=1)
    df_right = df_right.drop(columns_to_exclude, axis=1)

    # Remove unnamed columns
    df_left = df_left.loc[:, ~df_left.columns.str.contains('^Unnamed')]
    df_right = df_right.loc[:, ~df_right.columns.str.contains('^Unnamed')]

    # Remove duplicates row
    if remove_duplicates:
        df_left = df_left.drop_duplicates()
        df_right = df_right.drop_duplicates()

    # Generate the diff report
    df_report = df_left.merge(df_right,
                              on=df_right.columns.to_list(),
                              how=how,
                              indicator=True)

    if how != 'inner':
        # Remove common row ('both') from df
        df_report = df_report.loc[(df_report['_merge'] != 'both')]

    # Remove '_merge' column and return
    return df_report.drop('_merge', axis=1)


@strict_types
def write_sheet_to_excel(file_name: str, df: pd.DataFrame, sheet_name: str = 'Sheet1', sheet_header: bool = True):
    """
    Write a single sheet to an Excel file
    Args:
        file_name: the file name
        df: the source data frame
        sheet_name: (optional) the sheet name
        sheet_header: (optional) the option header for the sheet
    """
    write_sheets_to_excel(file_name, list([df]), list([sheet_name]), list([sheet_header]))


@strict_types
def write_sheets_to_excel(file_name: str,
                          df_list: list,
                          sheet_name_list: list | None = None,
                          sheet_header_list: list | None = None):
    """
    Write multi sheet to an Excel file
    Args:
        file_name: the file name
        df_list: the list of source data frame
        sheet_name_list: (optional) the list of sheet name
        sheet_header_list: (optional) the list of header option for each sheet
    """
    sheet_name_list = [] if sheet_name_list is None else sheet_name_list
    sheet_header_list = [] if sheet_header_list is None else sheet_header_list

    if not sheet_name_list:
        index_of_sheet = 0
        for index, _ in enumerate(df_list):
            if not df_list[index].empty:
                index_of_sheet += 1
            sheet_name_list.append("Sheet" + str(index_of_sheet))

    if not sheet_header_list:
        for _ in range(len(df_list)):
            sheet_header_list.append(True)

    if (len(df_list) != len(sheet_name_list)) | (len(sheet_header_list) != len(sheet_name_list)):
        logger.error("ERROR!! Length values of df '%d', sheet name '%d' and sheet_header "
                     "'%d' must be the same.", len(df_list), len(sheet_name_list), len(sheet_header_list))
        sys.exit(1)

    with pd.ExcelWriter(file_name, engine='xlsxwriter') as writer:
        for actual_df, actual_sheet_name, actual_sheet_header in zip(df_list, sheet_name_list, sheet_header_list):
            if not actual_df.empty:
                set_auto_filter_and_column_size_on_sheet(writer, actual_df, actual_sheet_name,
                                                         sheet_header=actual_sheet_header)


@strict_types
def set_auto_filter_and_column_size_on_sheet(writer: pd.io.excel._xlsxwriter.XlsxWriter, df: pd.DataFrame,
                                             sheet_name: str, max_column_length: int = MAX_COLUMN_LENGTH,
                                             selector_size_length: int = SELECTOR_SIZE, sheet_index: bool = False,
                                             sheet_header: bool = True) -> pd.ExcelWriter:
    """
    Set auto filter and width on all the df columns
    Args:
        writer: the Excel writer
        df: the source data frame
        sheet_name: the sheet name
        max_column_length: (optional) max column length
        selector_size_length: (optional) length of selector on column header
        sheet_index: (optional) True if the column index must be present
        sheet_header: (optional) False if the column header not be set

    Returns:
        the ExcelWriter
    """
    df.to_excel(writer, sheet_name=sheet_name, index=sheet_index,
                header=sheet_header)

    # Get xlsxwriter worksheet objects
    worksheet = writer.sheets[sheet_name]

    # Get dataframe dimensions
    (max_row, max_col) = df.shape

    if sheet_header:
        # Set auto filter
        worksheet.autofilter(0, 0, max_row, max_col - 1)

    # Set column length
    for column in df:
        col_idx = df.columns.get_loc(column)
        column_length = min(
            max(df[column].astype(str).map(len).max(), len(str(column))),
            max_column_length) + selector_size_length
        writer.sheets[sheet_name].set_column(col_idx, col_idx, column_length)

    return writer


@strict_types
def delete_files(*filenames: str) -> None:
    """
    Delete a list of files

    Args:
        *filenames: the file names
    """
    for filename in filenames:
        try:
            os.remove(filename)
        except OSError:
            pass


@strict_types
def check_python_and_pandas_version(python_version: str,
                                    minimal_python_version: str,
                                    pandas_version: str,
                                    minimal_pandas_version: str):
    """
    Check if the python and pandas version in use equal or greater than
    the minimal required. If not, a warning message is displayed.
    Args:
        python_version: Python version in use
        minimal_python_version: Minimal Python version required
        pandas_version: Pandas version in use
        minimal_pandas_version: Minimal Pandas version required
    """
    if not python_version.startswith(minimal_python_version):
        logger.warning(
            colour_text(MyColours.YELLOW, "WARNING! Script validated against python version "
                                          f"< {minimal_python_version} it is not guaranteed to work properly"))
    if not pandas_version.startswith(minimal_pandas_version):
        logger.warning(
            colour_text(MyColours.YELLOW, "WARNING! Script validated against pandas version "
                                          f"< {minimal_pandas_version} it is not guaranteed to work properly"))
