"""This module contains the functions to download the ER related fields for all the Vulnerability issues.

This module contains the functions that allow an authorized user to download
the ER related fields for all Vulnerability issues that are present in Jira.
After the download some ER fields are merged and the results are saved to an Excel file
and to several text files containing 'Vulnerability Status' and 'Status info details'
description.
"""

import sys
import os
import argparse
import time
import ntpath
import re
import shutil
from typing import Optional, get_type_hints
import pandas as pd
import jira

# CONSTANTS
ER_TXT_FOLDER = 'ER_DESCRIPTION_FILES'
IDENTIFIER_COLUMN_NAME = 'Identifier'
VULNERABILITY_STATUS_COLUMN_NAME = 'Vulnerability Status'
STATUS_INFO_COLUMN_NAME = 'Status info'
MITIGATION_ACTION_COLUMN_NAME = 'Mitigation action'
PLANNED_FIX_COLUMN_NAME = 'Planned Fix'
ER_REPORT_COLUMN_GROUP = [IDENTIFIER_COLUMN_NAME, VULNERABILITY_STATUS_COLUMN_NAME]
MERGED_STATUS_INFO_COLUMN_NAME = 'Merge Status info values'
RATIONAL_FOR_STATUS_EMPTY_VALUE = 'Empty field'
MERGED_MITIGATION_ACTION_COLUMN_NAME = 'Merge Mitigation action values'
MERGED_RESIDUAL_RISK_COLUMN_NAME = 'Merge Residual Risk values'
MERGED_PLANNED_FIX_COLUMN_NAME = 'Merge Planned Fix values'
DF_MERGED_COLUMNS_NAME = [VULNERABILITY_STATUS_COLUMN_NAME, MERGED_STATUS_INFO_COLUMN_NAME,
                          MERGED_MITIGATION_ACTION_COLUMN_NAME, MERGED_RESIDUAL_RISK_COLUMN_NAME,
                          MERGED_PLANNED_FIX_COLUMN_NAME]
VULNERABILITY_STATUS_PRIORITY_LIST = [
    'Untreated', 'Mitigated', 'Not applicable', 'False positive', 'Fixed'
]
SEVERITY_PRIORITY_LIST = ['Critical', 'High', 'Medium', 'Low']
RESIDUAL_RISK_DEFAULT_VALUE = 'Not Evaluated'
RESIDUAL_RISK_PRIORITY_LIST = [RESIDUAL_RISK_DEFAULT_VALUE] + SEVERITY_PRIORITY_LIST
JIRA_SERVER_ADDRESS = 'https://eteamproject.internal.ericsson.com'
JIRA_DEFAULT_TKN_NAME = "JIRA_AUTH_TOKEN"

JOIN_SEPARATOR = '\n--------\n'

MAX_COLUMN_LENGTH = 125
SELECTOR_SIZE = 5


def strict_types(function_to_check):
    """
    Strict function parameter type checker.

    To be added using @strict_type annotation
    It doesn't validate return type.
    Arguments:
        function_to_check: function name
    Returns:
        None if parameter type match the declared one, Throw exception otherwise
    """
    def type_checker(*args, **kwargs):
        hints = get_type_hints(function_to_check)

        all_args = kwargs.copy()
        all_args.update(dict(zip(function_to_check.__code__.co_varnames, args)))

        for _, value in enumerate(all_args):
            if value in hints and not isinstance(all_args[value], hints[value]):
                raise Exception(f"Type of '{value}' is '{hints[value]}' and not '{type(all_args[value])}'")
        return function_to_check(*args, **kwargs)

    return type_checker


@strict_types
def clean_dir_tree(new_dir: str):
    """
    Remove all files (directory tree) specified.

    Arguments:
        new_dir: a string - just directory name to be created in current execution directory
    Returns:
        None
    """
    shutil.rmtree(new_dir, ignore_errors=True)


@strict_types
def create_working_dir_and_remove_tree_if_exists(working_dir_name: str, remove_dirtree_if_exist: bool = True) -> str:
    """
    Create a directory and remove dirtree if requested.

    Create a directory. If the directory already exist and remove_dirtree_if_exist = True
    the directory an all the files in are deleted.
    Arguments:
        working_dir_name: a string - just directory name to be created in current execution directory
        remove_dirtree_if_exist: an integer
    Returns:
        The newly created directory or the existing one
    """
    new_dir = os.path.join(os.getcwd(), working_dir_name)
    if os.path.exists(new_dir) and os.path.isdir(new_dir):
        if remove_dirtree_if_exist:
            clean_dir_tree(new_dir)

    os.makedirs(new_dir, exist_ok=True)
    return new_dir


@strict_types
def set_auto_filter_and_column_size_on_sheet(writer: pd.io.excel._xlsxwriter.XlsxWriter,
                                             df: pd.DataFrame,
                                             sheet_name: str,
                                             max_column_length: int = MAX_COLUMN_LENGTH,
                                             selector_size_length: int = SELECTOR_SIZE,
                                             sheet_index: bool = False,
                                             sheet_header: bool = True) -> pd.ExcelWriter:
    """
    Set auto filter and width on all the df columns.

    Args:
        writer: the Excel writer
        df: the source data frame
        sheet_name: the sheet name
        max_column_length: (optional) max column length
        selector_size_length: (optional) length of selector on column header
        sheet_index: (optional) True if the column index must be present
        sheet_header: (optional) False if the column header not be set

    Returns:
        the ExcelWriter
    """
    df.to_excel(writer, sheet_name=sheet_name, index=sheet_index,
                header=sheet_header)

    # Get xlsxwriter worksheet objects
    worksheet = writer.sheets[sheet_name]

    # Get dataframe dimensions
    (max_row, max_col) = df.shape

    if sheet_header:
        # Set auto filter
        worksheet.autofilter(0, 0, max_row, max_col - 1)

    # Set column length
    for column in df:
        col_idx = df.columns.get_loc(column)
        column_length = min(
            max(df[column].astype(str).map(len).max(), len(str(column))),
            max_column_length) + selector_size_length
        writer.sheets[sheet_name].set_column(col_idx, col_idx, column_length)

    return writer


@strict_types
def validate_excel_file_name(file_name: str) -> str:
    """
    Check that the given file name is a valid Excel file name.

    Args:
        file_name: the string containing the full name of a file.

    Returns:
        the same file name with lowercase extension if it is a valid name,
        otherwise it raises an exception
    """
    path, ext = os.path.splitext(file_name)
    if path and ext.lower().endswith('.xlsx'):
        return path + ext.lower()
    raise argparse.ArgumentTypeError(
            f"not a valid Excel file name: {file_name}")


@strict_types
def exit_if_file_exists(file_name: str) -> None:
    """
    Verify that a file specified by the input parameter does not exist.

    If it finds one, the script it exits with an error code.

    Args:
        file_name: the string containing the full name of a file.
    """
    if os.path.isfile(file_name):
        print(f'\nOutput file "{file_name}" '
              'already exists, please remove it or change its name.\n')
        sys.exit(1)


@strict_types
def get_all_exemption_requests_from_jira(jira_token: str,
                                         jira_user: str,
                                         jira_password: str) -> pd.DataFrame:
    """
    Get info from the ER available in Jira.

    Downloads ER related fields for all Jira Vulnerabilities that satisfy
    a given query and converts them to a Pandas dataframe.
    Currently the following fields are requested:
    Args:
        jira_token: the Jira token value.
        jira_user: the Jira username.
        jira_password: the Jira password.

    Returns:
        a Dataframe containing ER data

    Summary -> Summary
    type: com.atlassian.jira.plugin.system.customfieldtypes:textfield
    Creator -> id: creator
    Team Name -> id: customfield_19813
    type: com.atlassian.jira.plugin.system.customfieldtypes:multiselect
    RA -> id: customfield_18644
    type: com.atlassian.jira.plugin.system.customfieldtypes:select
    Labels -> id: labels
    type: array of strings
    Component/s -> id: components
    type: array of component
    Status -> id: status
    Resolution-> id: resolution
    Sprint -> id: customfield_11910
    type: array of strings
    Product Name -> id: customfield_14019
    type: com.atlassian.jira.plugin.system.customfieldtypes:textfield
    Product Number -> id: customfield_24755
    type: com.atlassian.jira.plugin.system.customfieldtypes:textfield
    Severity -> id: customfield_12651
    type: com.atlassian.jira.plugin.system.customfieldtypes:select
    Residual Risk -> id: customfield_24418
    type: com.atlassian.jira.plugin.system.customfieldtypes:select
    Tool -> id: customfield_51221
    type: com.atlassian.jira.plugin.system.customfieldtypes:textfield
    Identifier -> id: customfield_51748
    type: com.atlassian.jira.plugin.system.customfieldtypes:textfield
    Vulnerability description -> id: customfield_51216
    type: com.atlassian.jira.plugin.system.customfieldtypes:textarea
    Vulnerability Status -> id: customfield_51753
    type: com.atlassian.jira.plugin.system.customfieldtypes:select
    Status info -> id: customfield_51779
    type: com.atlassian.jira.plugin.system.customfieldtypes:textarea
    Mitigation action -> id: customfield_15551
    type: com.atlassian.jira.plugin.system.customfieldtypes:textarea
    Planned Fix -> id: customfield_13624
    type: com.atlassian.jira.plugin.system.customfieldtypes:textfield
    Expected Mitigation date -> id: customfield_30127
    type: com.atlassian.jira.plugin.system.customfieldtypes:datepicker
    """
    jira_options = {'server': JIRA_SERVER_ADDRESS}
    list_field_separator = '|'

    def get_name(issue_field: Optional[dict]) -> Optional[str]:
        """
        Get the value for the 'name' key of a dictionary field.

        Args:
            the dictionary field containing the 'name': value item

        Returns:
            a string containing the value of the 'name' key
        """
        return issue_field['name'] if issue_field else None

    def get_resolution(issue_field: Optional[dict]) -> Optional[str]:
        """
        Get the value for the 'name' key  of the resolution field.

        Args:
            the resolution dictionary field

        Returns:
            a string containing the value of the 'name' key, or 'Unresolved'
            in case the resolution field is None
        """
        # the default choice for Resolution field is "Unresolved" and
        # in this case an empty field is returned from Jira. A remapping
        # to "Unresolved" is required in favour of visualization.
        return get_name(issue_field) if issue_field else "Unresolved"

    def get_value(issue_field: Optional[dict]) -> Optional[str]:
        """
        Get the value for the 'value' key of a dictionary field.

        Args:
            the field containing the 'value': value item

        Returns:
            a string containing the value of the 'value' key
        """
        return issue_field['value'] if issue_field else None

    def get_joined_names(issue_field: Optional[list]) -> Optional[str]:
        """
        Join the values for the 'name' key of a dictionary list field.

        Join the values for the 'name' key of a dictionary list field.
        Args:
            the list field containing the 'name': value items

        Returns:
            a string containing the concatenation of the values of the 'name' key
        """
        return list_field_separator.join(d['name'] for d in issue_field) if issue_field else None

    def get_joined_values(issue_field: Optional[list]) -> Optional[str]:
        """
        Join the values for the 'value' key of a dictionary list field.

        Join the values for the 'value' key of a dictionary list field.
        Args:
            the dictionary list field containing the 'value': value items

        Returns:
            a string containing the concatenation of the values of the 'value' key
        """
        return list_field_separator.join(d['value'] for d in issue_field) if issue_field else None

    def get_joined_sprint_values(issue_field: Optional[list]) -> Optional[str]:
        """
        Join the sprint values extracted from the Sprint field.

        Join the sprint values extracted from the Sprint field.
        Args:
            the Sprint field, in the form of a list of strings representing sprints

        Returns:
            a string containing the concatenation of the values of the sprint names
        """
        def extract_value_from_sprint_field(sprint_field_item: str) -> str:
            """
            Extract value from sprint field.

            Extracts just the name of the Sprint from the complex structure
            contained in the string related to a Sprint field element.

            Args:
                sprint_field_item: a single item contained in the Sprint field

            Returns:
                a string containing the sprint name
            """
            name_search = re.search(r'name\s*=\s*([^,]+)', sprint_field_item)
            sprint_value = ""
            if name_search is not None:
                name_value = name_search.group(1)
                sprint_value = name_value.strip()
            return sprint_value

        if issue_field is not None:
            return list_field_separator.join(
                extract_value_from_sprint_field(d) for d in issue_field)

    dispatch_issue_fields = {
        "summary": None,
        "creator": get_name,
        "customfield_19813": get_joined_values,
        "customfield_18644": get_value,
        "labels": list_field_separator.join,
        "components": get_joined_names,
        "status": get_name,
        "resolution": get_resolution,
        "customfield_11910": get_joined_sprint_values,
        "customfield_14019": None,
        "customfield_24755": None,
        "customfield_12651": get_value,
        "customfield_24418": get_value,
        "customfield_51221": None,
        "customfield_51748": None,
        "customfield_51216": None,
        "customfield_51753": get_value,
        "customfield_51779": None,
        "customfield_15551": None,
        "customfield_13624": None,
        "customfield_30127": None
    }

    @strict_types
    def adjust_response_issues(response_issues: list):
        """
        Adjust response issues.

        Modifies the values in the received response issues
        so that for each field key the value will be taken from the field contents
        according to the specific field type schema

        Args:
            response_issues: the 'response.issues' json data
        Returns:
            a dataframe containing the processed response data
        """
        for issue in response_issues:
            issue_fields = issue['fields']
            for key in issue_fields.keys():
                if dispatch_issue_fields[key] is not None:
                    issue_fields[key] = dispatch_issue_fields[key](
                        issue_fields[key])

    try:
        if jira_token:
            jira_instance = jira.JIRA(options=jira_options,
                                      token_auth=jira_token)
        else:
            jira_instance = jira.JIRA(options=jira_options,
                                      basic_auth=(jira_user, jira_password))
    except Exception as exc:
        print(f'\nException on open connection to JIRA server: {exc}\n')
        sys.exit(1)

    try:
        all_fields = jira_instance.fields()
    except jira.JIRAError as exc:
        print(f'\nException on reading JIRA fields from server: {exc}\n')
        sys.exit(2)

    all_fields_id_name_map = {field['id']: field['name']
                              for field in all_fields}

    for field_id in dispatch_issue_fields:
        if field_id not in all_fields_id_name_map.keys():
            print(f"ERROR! The field '{field_id}' is not available in JIRA!")
            # print(f"\nList of keys and value:\n{all_fields_id_name_map}")
            sys.exit(1)

    jql_query_for_vuln = 'project=\"TOR Project\" '\
        'AND issueType = Vulnerability '\
        'AND component = CVE_ISSUE_AUTO_TASK '\
        'AND identifier ~ \"CVE-*-*"'\
        'order by created desc'

    df_all_vuln = pd.DataFrame()

    fields_of_interest = [
        'fields.' + field_id for field_id in dispatch_issue_fields
    ]

    request_fields = list(dispatch_issue_fields.keys())
    batch_size = 100
    total_got = 0
    try:
        response = jira_instance.search_issues(jql_query_for_vuln,
                                               maxResults=batch_size,
                                               startAt=0,
                                               fields=request_fields,
                                               json_result=True)

        total_vuln_to_be_read = int(response['total'])

        if total_vuln_to_be_read == 0:
            print('\nNo Jira Vulnerability issues have been found with'
                  ' the required properties.\n')
            return pd.DataFrame()

        print(f'\nER fields for {total_vuln_to_be_read} vulnerability issues '
              f'shall be downloaded from Jira in batches of {batch_size}.\n')
        issues = response['issues']
        adjust_response_issues(issues)
        df_partial = pd.json_normalize(issues)
        df_all_vuln = pd.concat(
            [df_all_vuln, df_partial[['key'] + fields_of_interest]])

        total_got += len(issues)

        while total_got < total_vuln_to_be_read:
            ten_times_char = '.' if total_got % (batch_size * 10) else '*'
            print(ten_times_char, end='', flush=True)
            time.sleep(0.5)
            response = jira_instance.search_issues(jql_query_for_vuln,
                                                   maxResults=batch_size,
                                                   startAt=total_got,
                                                   fields=request_fields,
                                                   json_result=True)
            issues = response['issues']
            got = len(issues)
            adjust_response_issues(issues)
            df_partial = pd.json_normalize(issues)
            df_all_vuln = pd.concat(
                [df_all_vuln, df_partial[['key'] + fields_of_interest]])
            total_got += got

        print()
        df_all_vuln.rename(
            columns={'fields.' + field_id: all_fields_id_name_map[field_id]
                     for field_id in all_fields_id_name_map.keys()},
            inplace=True)
        df_all_vuln['key'] = df_all_vuln['key'].apply(
            lambda x: f'= HYPERLINK("{JIRA_SERVER_ADDRESS}/browse/{x}", "{x}")')
    except Exception as exc:
        print('\nException caught while getting Vulnerability '
              f'data from JIRA server: \n{exc}\n')
        return pd.DataFrame()

    return df_all_vuln


@strict_types
def merge_rational_for_status_fields(df: pd.DataFrame) -> pd.DataFrame:
    """
    Group by 'ER_REPORT_GROUP_RESTRICTED_FIELDS' and merge 'Rational for status' values.

    Args:
        df: ER report data frame
    """
    merge_rfs_df = df.copy()
    merge_rfs_df[STATUS_INFO_COLUMN_NAME] = (merge_rfs_df[STATUS_INFO_COLUMN_NAME]
                                             .fillna(value=RATIONAL_FOR_STATUS_EMPTY_VALUE))
    merge_rfs_df[STATUS_INFO_COLUMN_NAME] = merge_rfs_df[STATUS_INFO_COLUMN_NAME].apply(lambda x: str(x).strip())
    merge_rfs_df = merge_rfs_df.drop_duplicates(subset=[IDENTIFIER_COLUMN_NAME,
                                                        VULNERABILITY_STATUS_COLUMN_NAME,
                                                        STATUS_INFO_COLUMN_NAME])

    rfs_df = merge_rfs_df \
        .groupby(ER_REPORT_COLUMN_GROUP, group_keys=True)[STATUS_INFO_COLUMN_NAME] \
        .apply(JOIN_SEPARATOR.join) \
        .reset_index(name=MERGED_STATUS_INFO_COLUMN_NAME).copy()

    return get_max_severity_records(rfs_df, VULNERABILITY_STATUS_PRIORITY_LIST)


@strict_types
def merge_mitigation_action_fields(df: pd.DataFrame) -> pd.DataFrame:
    """
    Group by 'ER_REPORT_GROUP_RESTRICTED_FIELDS' and merge 'MITIGATION_ACTION_COLUMN_NAME' values.

    Args:
        df: ER report data frame
    """
    merge_mp_df = df.copy()
    merge_mp_df[MITIGATION_ACTION_COLUMN_NAME] = \
        merge_mp_df[MITIGATION_ACTION_COLUMN_NAME].apply(lambda x: str(x).strip())
    merge_mp_df = df.drop_duplicates(
        subset=[IDENTIFIER_COLUMN_NAME, VULNERABILITY_STATUS_COLUMN_NAME, MITIGATION_ACTION_COLUMN_NAME])
    mp_not_na_df = merge_mp_df.loc[~merge_mp_df[MITIGATION_ACTION_COLUMN_NAME].isna()] \
        .dropna(subset=[MITIGATION_ACTION_COLUMN_NAME]) \
        .groupby(ER_REPORT_COLUMN_GROUP, group_keys=True)[MITIGATION_ACTION_COLUMN_NAME] \
        .apply(JOIN_SEPARATOR.join) \
        .reset_index(name=MERGED_MITIGATION_ACTION_COLUMN_NAME).copy()

    mp_na_df = merge_mp_df.loc[merge_mp_df[MITIGATION_ACTION_COLUMN_NAME].isna()] \
        .drop_duplicates().copy()

    # Remove useless columns
    mp_na_df = mp_na_df[ER_REPORT_COLUMN_GROUP]

    result_df = pd.concat([mp_not_na_df, mp_na_df])
    return get_max_severity_records(result_df,
                                    VULNERABILITY_STATUS_PRIORITY_LIST)


@strict_types
def get_higher_residual_risk(df: pd.DataFrame, merged_df: pd.DataFrame,
                             residual_risk_priority_list: list) -> pd.DataFrame:
    """
    Get the higher residual risk value for the entries with the same CVE identifier.

    Args:
        df: JIRA dataframe
        merged_df: merged dataframe
        residual_risk_priority_list: list of residual risk sorted by priority

    Returns:
        merged dataframe with the higher priority residual risk value filled for every CVE identifier
    """
    # Add column name with default value on the merged dataframe
    merged_df.loc[:, MERGED_RESIDUAL_RISK_COLUMN_NAME] = RESIDUAL_RISK_DEFAULT_VALUE
    # Remove unused columns
    # merged_df = merged_df[ER_REPORT_COLUMN_GROUP + [MERGED_RESIDUAL_RISK_COLUMN_NAME]]

    for merged_df_index, merged_df_row in merged_df.iterrows():
        found_df = df.loc[
            (df[IDENTIFIER_COLUMN_NAME] == merged_df_row[IDENTIFIER_COLUMN_NAME]) &
            (df[VULNERABILITY_STATUS_COLUMN_NAME] != 'Not applicable') &
            (df[VULNERABILITY_STATUS_COLUMN_NAME] != 'False positive')
        ].copy()
        if found_df.empty:
            merged_df.at[merged_df_index, MERGED_RESIDUAL_RISK_COLUMN_NAME] = 'None'
        else:
            found_value = False
            for severity in residual_risk_priority_list:
                for _, found_df_row in found_df.iterrows():
                    if severity == found_df_row['Residual Risk']:
                        merged_df.at[merged_df_index, MERGED_RESIDUAL_RISK_COLUMN_NAME] = severity
                        found_value = True
                        break

                if found_value:
                    break

    return merged_df


@strict_types
def merge_planned_fix_fields(er_df: pd.DataFrame, df: pd.DataFrame) \
        -> pd.DataFrame:
    """
    Aggregate the planned fix values for the entries on the df dataframe.

    Args:
        er_df: exemption request dataframe
        df: dataframe with values to search
    """
    result_df = pd.DataFrame()
    result_df[ER_REPORT_COLUMN_GROUP] = [None, None]

    pf_df = er_df.loc[~er_df[PLANNED_FIX_COLUMN_NAME].isna()].copy()
    # Convert column value to string
    pf_df[PLANNED_FIX_COLUMN_NAME] = pf_df[PLANNED_FIX_COLUMN_NAME].astype('string')
    pf_df[PLANNED_FIX_COLUMN_NAME] = pf_df[PLANNED_FIX_COLUMN_NAME].apply(lambda x: str(x).strip())

    er_no_duplicate_df = pf_df.drop_duplicates(
        subset=[IDENTIFIER_COLUMN_NAME, VULNERABILITY_STATUS_COLUMN_NAME, PLANNED_FIX_COLUMN_NAME])
    for _, row in df.iterrows():
        found_df = er_no_duplicate_df.loc[
                (er_no_duplicate_df[IDENTIFIER_COLUMN_NAME] == row[IDENTIFIER_COLUMN_NAME])
                & (er_no_duplicate_df[VULNERABILITY_STATUS_COLUMN_NAME] ==
                   row[VULNERABILITY_STATUS_COLUMN_NAME])]

        pf_not_na_df = found_df.loc[~found_df[PLANNED_FIX_COLUMN_NAME].isna()] \
            .groupby(ER_REPORT_COLUMN_GROUP, group_keys=True)[PLANNED_FIX_COLUMN_NAME] \
            .apply(JOIN_SEPARATOR.join) \
            .reset_index(name=MERGED_PLANNED_FIX_COLUMN_NAME).copy()

        result_df = pd.concat([result_df, pf_not_na_df])

    return result_df


@strict_types
def get_max_severity_records(df: pd.DataFrame,
                             severity_priority_list: list) -> pd.DataFrame:
    """
    Drop row according to Vulnerability Status max severity.

    Args:
        df: ER report data frame
        severity_priority_list: severity priority list for ER vulnerability
    """
    unique_identifier_list = df[IDENTIFIER_COLUMN_NAME].unique()
    result_df = df.copy()
    result_df = result_df.iloc[0:0]
    for identifier in unique_identifier_list:
        for severity in severity_priority_list:
            found_df = df.loc[(df[IDENTIFIER_COLUMN_NAME] == identifier)
                              & (df[VULNERABILITY_STATUS_COLUMN_NAME] == severity)]
            found_df = found_df.drop_duplicates(ER_REPORT_COLUMN_GROUP,
                                                keep='first')
            if not found_df.empty:
                result_df = pd.concat([result_df, found_df])
                break

    return result_df


@strict_types
def write_content_to_file(row: pd.Series, column_name: str, file_descriptor):
    """
    Write the field identified by the column name of the specific row to a file.

    Args:
        row: the dataframe row
        column_name: the column name
        file_descriptor: the file descriptor
    """
    if isinstance(row[column_name], str):
        file_descriptor.write(row[column_name])


@strict_types
def save_text_files(df: pd.DataFrame, folder_name: str, column_name: str,
                    post_fix: str):
    """
    Save on a txt file all the 'Merge Status info' field for every 'IDENTIFIER_COLUMN_NAME'.

    Args:
        df: vulnerability status report data frame
        folder_name: folder name to save files
        column_name: column name
        post_fix: file name postfix
    """
    create_working_dir_and_remove_tree_if_exists(folder_name, False)
    print()
    error_number = 0
    for _, row in df.iterrows():
        normalize_identifier = row[IDENTIFIER_COLUMN_NAME].replace(' ', '_').replace('/', '_')
        file_name = f'{folder_name}/{normalize_identifier}{post_fix}.txt'
        try:
            with open(file_name, 'w', encoding='utf-8') as txt_file:
                write_content_to_file(row, column_name, txt_file)
            print(f"Generated file: {file_name}")
        except (FileNotFoundError, IOError) as exc_error:
            error_number += 1
            print(f"\nError on Identifier '{row[IDENTIFIER_COLUMN_NAME]}' to create "
                  f"file name '{file_name}'.")
            print(f"Detailed reason: {exc_error}\n")

    print(f"Total generated file/s: {len(df) - error_number}")


@strict_types
def save_text_merged_files(merged_df: pd.DataFrame, folder_name: str):
    """
    Save all the text files for 'Rational for status' and 'MITIGATION_ACTION_COLUMN_NAME' values.

    Args:
        merged_df: Vulnerability merged dataframe
        folder_name: folder name to save files
    """
    if merged_df.empty:
        print('\nDataframe is empty, no text file will be created.\n')
        return

    create_working_dir_and_remove_tree_if_exists(folder_name)
    save_text_files(merged_df, folder_name,
                    MERGED_STATUS_INFO_COLUMN_NAME, '_rfs')
    save_text_files(merged_df, folder_name,
                    MERGED_MITIGATION_ACTION_COLUMN_NAME, '_mp')


@strict_types
def merge_jira_vulnerability_fields(df: pd.DataFrame) -> pd.DataFrame:
    """
    Merge the jira vulnerability fields.

    Args:
        df: Jira vulnerability extracted dataframe
    Returns:
        dataframe with vulnerability merged fields
    """
    if df.empty:
        print('\nDataframe is empty, no merge of vulnerability fields.\n')
        return df

    # Rational for status management
    rational_for_status_df = merge_rational_for_status_fields(df)
    # Mitigation action management
    mitigation_plan_df = merge_mitigation_action_fields(df)
    # Merge to result dataframe
    merged_df = pd.merge(rational_for_status_df,
                         mitigation_plan_df,
                         how='inner',
                         on=ER_REPORT_COLUMN_GROUP,
                         validate='m:m')

    # Residual risk management
    merged_df = get_higher_residual_risk(df,
                                         merged_df,
                                         RESIDUAL_RISK_PRIORITY_LIST)

    # Planned Fix management
    planned_fix_df = merge_planned_fix_fields(df, merged_df)
    # Merge to result dataframe
    if not planned_fix_df.empty:
        merged_df = pd.merge(merged_df,
                             planned_fix_df,
                             how='outer',
                             on=ER_REPORT_COLUMN_GROUP,
                             validate='m:m')
    else:
        merged_df.loc[:, MERGED_PLANNED_FIX_COLUMN_NAME] = ''

    return merged_df


@strict_types
def save_er_to_xlsx(er_dataframe: pd.DataFrame,
                    merged_er_dataframe: pd.DataFrame,
                    output_file_name: str) -> None:
    """
    Save the ER dataframe to an Excel file.

    Args:
        er_dataframe: the Dataframe that shall be saved
        merged_er_dataframe: merged_df: Vulnerability merged dataframe
        output_file_name: the full name of the output file where the ER data shall be saved
    """
    if er_dataframe.empty:
        print('\nDataframe is empty, no file will be created.\n')
        return

    try:
        with (pd.ExcelWriter(output_file_name, engine='xlsxwriter')
              as writer):  # pylint: disable=abstract-class-instantiated
            er_sheet_name = 'ER Report'
            er_dataframe.to_excel(writer,
                                  sheet_name=er_sheet_name,
                                  header=True,
                                  index=False)
            set_auto_filter_and_column_size_on_sheet(
                writer, er_dataframe, er_sheet_name)

            merged_er_fields_sheet_name = 'Merged ER fields'
            merged_er_dataframe.to_excel(
                writer,
                sheet_name=merged_er_fields_sheet_name,
                header=True,
                index=False)
            set_auto_filter_and_column_size_on_sheet(
                writer, merged_er_dataframe, merged_er_fields_sheet_name)

            url_format = writer.book.get_default_url_format()  # pylint: disable=no-member
            writer.sheets[er_sheet_name].set_column(0, 0, 12, url_format)
    except Exception as exc:
        print(
            f'\nException caught while saving ER data to Excel file: \n{exc}\n'
        )


@strict_types
def merge_status_info_values(closed_after_fix_df: pd.DataFrame, jira_df: pd.DataFrame) -> pd.DataFrame:
    """Merge status info value on dataframe.

    Args:
        closed_after_fix_df (pd.DataFrame): dataframe containing closed Jira after fix
        jira_df (pd.DataFrame): Jira dataframe

    Returns:
        pd.DataFrame: dataframe with merged status info
    """
    for index, row in closed_after_fix_df.iterrows():
        jira_temp_df = jira_df[(jira_df.Identifier == row[IDENTIFIER_COLUMN_NAME]) &
                               (jira_df.Status == 'Closed') &
                               (jira_df.Resolution == 'Invalid') &
                               (~pd.isna(jira_df['Status info']))]

        status_info_values_set = set()
        if jira_temp_df.empty:
            closed_after_fix_df.at[index, MERGED_STATUS_INFO_COLUMN_NAME] = ''
        else:
            for _, jira_temp_row in jira_temp_df.iterrows():
                status_info_values_set.add(str(jira_temp_row[STATUS_INFO_COLUMN_NAME]).strip())

        all_values = ''
        items = len(status_info_values_set)
        for value in status_info_values_set:
            all_values += str(value)
            if items > 1:
                all_values += "\n----\n"

        closed_after_fix_df.at[index, MERGED_STATUS_INFO_COLUMN_NAME] = all_values

    return closed_after_fix_df


@strict_types
def add_closed_cve_to_merged_df(jira_df: pd.DataFrame,
                                jira_not_closed_df: pd.DataFrame,
                                merged_df: pd.DataFrame) -> pd.DataFrame:
    """Add to a dataframe all the Jira ticket that have all the CVE instance in Closed state.

    Args:
        jira_df (pd.DataFrame): Jira dataframe
        jira_not_closed_df (pd.DataFrame): Jira dataframe without Closed ticket
        merged_df (pd.Dataframe): Merged dataframe

    Returns:
        pd.DataFrame: dataframe with the IDENTIFIER_COLUMN_NAME of all the CVE instance in Closed state
    """
    #
    # Managed Status == Closed and Resolution == Invalid
    #
    only_closed_invalid_df = jira_df[(jira_df.Status == 'Closed') & (jira_df.Resolution == 'Invalid')].copy()
    identifier_closed_after_fix_df = only_closed_invalid_df[IDENTIFIER_COLUMN_NAME].drop_duplicates().to_frame()
    identifier_not_closed_df = jira_not_closed_df[IDENTIFIER_COLUMN_NAME].drop_duplicates().to_frame()

    identifier_instance_closed_df = \
        (pd.merge(identifier_closed_after_fix_df, identifier_not_closed_df,
                  indicator=True, how='outer', on=IDENTIFIER_COLUMN_NAME, validate='1:m')
         .query('_merge=="left_only"')
         .drop('_merge', axis=1))

    # Add the merged columns to the identifier_instance_closed_df
    identifier_instance_closed_df[DF_MERGED_COLUMNS_NAME] = "- CLOSED INVALID CVE -"

    # Merge values on "Merged status info"
    identifier_instance_closed_df = merge_status_info_values(identifier_instance_closed_df, jira_df)

    merged_df = pd.concat([merged_df, identifier_instance_closed_df]).copy()

    #
    # Managed Status == Closed and Resolution != Invalid
    #
    only_closed_after_fix_df = jira_df[(jira_df.Status == 'Closed') & (jira_df.Resolution != 'Invalid')].copy()
    identifier_closed_after_fix_df = only_closed_after_fix_df[IDENTIFIER_COLUMN_NAME].drop_duplicates().to_frame()
    identifier_not_closed_df = jira_not_closed_df[IDENTIFIER_COLUMN_NAME].drop_duplicates().to_frame()

    identifier_instance_closed_df = \
        (pd.merge(identifier_closed_after_fix_df, identifier_not_closed_df,
                  indicator=True, how='outer', on=IDENTIFIER_COLUMN_NAME, validate='1:m')
         .query('_merge=="left_only"')
         .drop('_merge', axis=1))

    # Add the merged columns to the identifier_instance_closed_df
    identifier_instance_closed_df[DF_MERGED_COLUMNS_NAME] = "- CLOSED CVE -"

    merged_df = pd.concat([merged_df, identifier_instance_closed_df]).copy()
    merged_df = merged_df.drop_duplicates(IDENTIFIER_COLUMN_NAME, keep='first')

    return merged_df


def main():
    """Invoke when launched as standalone script.

    It reads from input the full name of the output file that will be generated,
    the Jira username and password if provided otherwise it use the env var
    containing the JIRA token. Invokes the function which gets the ER data from
    Jira, performs a merge of several fields and saves the results to an Excel
    file and several text files.
    """
    parser = argparse.ArgumentParser(
        description="Gets all Jira Vulnerability fields related to Exemption "
        "Requests, merges some of them and save an Excel file and several text "
        "files. To connect to JIRA it use an env var or the provided "
        f"user and password. The default env var is: {JIRA_DEFAULT_TKN_NAME}")
    parser.add_argument('-o',
                        '--out',
                        type=validate_excel_file_name,
                        required=True,
                        help="The full name of the Excel file (.xlsx) "
                        "that will store the processed ER fields.")
    parser.add_argument('-u',
                        '--user',
                        type=str,
                        required=False,
                        default="",
                        help="the Jira user name")
    parser.add_argument('-p',
                        '--password',
                        type=str,
                        required=False,
                        default="",
                        help="the Jira user password")
    parser.add_argument('-t',
                        '--token_name',
                        type=str,
                        required=False,
                        default=JIRA_DEFAULT_TKN_NAME,
                        help="the JIRA token name env variable")

    if len(sys.argv) == 1:
        parser.print_help()
        sys.exit(1)

    args = parser.parse_args()

    jira_tkn_value = os.getenv(args.token_name)
    if jira_tkn_value is None:
        print("\nNo Jira Basic Authentication credentials have been provided, "
              f"in the '{args.token_name}' env variable.\n"
              "Proceeding using user and password authentication method.\n")

        if args.user == '' or args.password == '':
            print("ERROR. No Jira user and password credentials have been provided. Program aborted!\n")
            sys.exit(1)

    exit_if_file_exists(args.out)

    jira_er_dataframe = get_all_exemption_requests_from_jira(jira_tkn_value, args.user, args.password)

    # Remove entries with Status==Closed from dataframe
    dframe = jira_er_dataframe[jira_er_dataframe.Status != "Closed"].copy()

    merged_dframe = merge_jira_vulnerability_fields(dframe)

    # Manage closed CVE instances
    merged_dframe = add_closed_cve_to_merged_df(jira_er_dataframe, dframe, merged_dframe)

    save_er_to_xlsx(jira_er_dataframe, merged_dframe, args.out)
    folder_name, _ = ntpath.split(args.out)
    save_text_merged_files(merged_dframe,
                           os.path.join(folder_name, ER_TXT_FOLDER))


if __name__ == "__main__":
    main()
